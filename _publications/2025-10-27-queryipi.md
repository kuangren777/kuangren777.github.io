---
title: "QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents"
collection: publications
category: preprints
permalink: /publication/2025-10-27-queryipi
excerpt: 'This paper introduces QueryIPI, the first query-agnostic indirect prompt injection attack for realistic coding agents. By leveraging leaked internal prompts and iterative optimization, it achieves high success rates in compromising agents regardless of user input. To appear in <b>arXiv</b>.'
date: 2025-10-27
venue: 'arXiv preprint arXiv:2510.23675'
paperurl: 'https://arxiv.org/abs/2510.23675'
citation: 'Yuchong Xie, Zesen Liu, <b>Mingyu Luo</b>, Zhixiang Zhang, Kaikai Zhang, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She. (2025). "QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents." <i>arXiv preprint arXiv:2510.23675</i>.'
---

## Abstract

Coding agents in modern IDEs orchestrate powerful tools and system-level actions, creating a high-stakes attack surface. Prior works on Indirect Prompt Injection (IPI) are mainly query-specific, rendering attacks unstable and dependent on specific user inputs. We argue that a query-agnostic attack represents a far more severe threat to real-world LLM coding agents. QueryIPI is the first query-agnostic IPI method for realistic coding agents, leveraging the agent's internal prompt to transform the attack into a constrained, white-box optimization problem. Using an iterative, prompt-based search, QueryIPI refines malicious tool descriptions to reliably trigger attacks regardless of user queries. Evaluations on five simulated realistic coding agents show an average success rate of up to 87%, significantly outperforming baselines. Crucially, these attacks successfully transfer to real-world agents, proving the practical threat of internal-prompt-aware, query-agnostic IPI.

## Motivation

* **Instability of Existing Attacks**: Current Indirect Prompt Injection (IPI) methods are largely *query-specific*, meaning the attack only succeeds if the user's query inadvertently triggers the malicious tool or content. This makes attacks opportunistic and unreliable.
* **The Query-Agnostic Threat**: A far more dangerous threat model is a *query-agnostic* attack, which triggers reliably regardless of what the user asks. However, achieving this is difficult because the malicious payload in a tool description must compete for the agent's attention against the user's actual query and system prompts.
* **Leveraging Internal Prompts**: The key insight is that this challenge can be overcome by exploiting the leakage of the agent's internal prompt. Access to the internal prompt allows attackers to tailor payloads that specifically target the agent's logic and guardrails, turning a black-box guessing game into a white-box optimization task.

## Methodology

QueryIPI employs an automated, iterative framework to generate malicious tool descriptions that reliably compromise the agent.

![Comparison of Query-Specific and Query-Agnostic Indirect Prompt Injection](/images/pub/queryipi/fig1.png)
*Figure 1: Comparison of Query-Specific (Left) vs. Query-Agnostic (Right) Indirect Prompt Injection. QueryIPI (Right) triggers the attack deterministically regardless of the user's input.*

The methodology consists of two main components:

1.  **Internal Prompt-Aware Optimization**:
    * **Initial Seed Generation**: The system uses the agent's leaked internal prompt to generate an initial "seed" tool description. This description is crafted to mimic the agent's native toolset style and align with its internal logic and safety constraints.
    * **Reflective Optimization**: The system iteratively refines the tool description. A "Mutation LLM" analyzes past failures (e.g., refusals or ignored tools) by cross-referencing them with the agent's internal prompt to identify specifically which guardrails were triggered and how to bypass them.

2.  **Iterative Search Algorithm**:
    * The process follows a genetic-style algorithm (Algorithm 1). In each generation, variants of the tool description are generated and evaluated against a set of diverse user queries.
    * A **Scoring Function** uses a calibrated "Judge LLM" to assess whether the agent executed the malicious command, providing a granular score (0-100) rather than a binary result to guide the optimization.

## Key Results

* **High Success Rate**: On five simulated agents (Cursor, Windsurf, Cline, Copilot, Trae), QueryIPI achieved an average Attack Success Rate (ASR) of **87%** when trained with 8 samples, compared to just 50% for the baseline.
* **Real-World Transferability**: Attacks optimized in the simulated environment successfully transferred to **real-world** versions of the coding agents. QueryIPI achieved an average ASR of **50%** on real agents, whereas the baseline achieved only 2%.
* **Robustness to Partial Knowledge**: Even without the exact internal prompt, QueryIPI achieved a **71%** success rate using only publicly available, partial prompts, demonstrating high robustness.
* **Stealth**: The generated malicious descriptions were stealthy enough to bypass common detection methods like Perplexity-based filters, with scores indistinguishable from benign tools.