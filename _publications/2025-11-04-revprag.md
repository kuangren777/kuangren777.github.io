---
title: "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis"
collection: publications
category: conferences
permalink: /publication/2025-11-04-revprag
excerpt: 'This paper introduces RevPRAG, a novel framework for detecting poisoning attacks in Retrieval-Augmented Generation (RAG) systems by analyzing the internal activations of Large Language Models (LLMs).
'
date: 2025-11-04
venue: 'Findings of the Association for Computational Linguistics: EMNLP 2025'
paperurl: 'https://aclanthology.org/2025.findings-emnlp.698.pdf'
citation: 'Xue Tan, Hao Luan, <b>Mingyu Luo</b>, Xiaoyan Sun, Ping Chen, and Jun Dai. 2025. RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 12999â€“13011, Suzhou, China. Association for Computational Linguistics.'
---

## Abstract

Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge, but this openness introduces a new attack surface: knowledge database poisoning. Attackers can inject malicious texts into the knowledge base to manipulate the LLM into generating specific, incorrect responses. Existing defense methods often rely on heavy external models or complex filtering that cannot effectively detect attacks during the generation process. In this work, we propose **RevPRAG**, a flexible and automated pipeline that detects RAG poisoning by leveraging the intrinsic activation patterns of LLMs. Our extensive experiments across various LLM architectures and datasets demonstrate that RevPRAG achieves a **98% True Positive Rate (TPR)** while maintaining a **False Positive Rate (FPR) close to 1%**, significantly outperforming existing baselines in both accuracy and efficiency.

## Motivation

* **Vulnerability in RAG**: While RAG allows models to access up-to-date information from sources like Wikipedia, it also allows attackers to inject "poisoned" content. If retrieved, this content can hijack the model's output (e.g., forcing the answer "Fuji" for the question "What is the highest mountain?").
* **Limitations of Current Defenses**: Current defense strategies (e.g., RobustRAG, INSTRUCTRAG) typically require additional large models for voting or denoising, resulting in high computational overhead and latency.
* **Key Insight**: We hypothesize and empirically verify that LLMs exhibit distinct internal activation patterns when generating a "poisoned" response compared to a "correct" response. By capturing these internal states, we can distinguish trustworthy outputs from manipulated ones without altering the RAG workflow.

![t-SNE visualizations of activations for correct and poisoned responses](/images/pub/revprag/fig2.png)
*Figure 1: t-SNE visualizations showing distinct clusters for correct vs. poisoned responses.*

## Methodology

![The workflow of RevPRAG](/images/pub/revprag/fig3.png)
*Figure 2: The complete workflow of the RevPRAG detection pipeline.*

RevPRAG operates through a three-stage pipeline:

1.  **Poisoning Data Collection**: We simulate real-world attacks using state-of-the-art methods (such as PoisonedRAG and GARAG) to generate poisoned texts and inject them into the knowledge database. This creates a dataset of "poisoned" vs. "clean" generation scenarios.
2.  **Activation Collection & Preprocessing**:
    * We extract the activations of the final token in the input sequence across all layers of the LLM.
    * These activations serve as a comprehensive summary of the model's decision-making process regarding the context and query.
    * The raw activations are normalized to ensure consistent input for the detection model.
3.  **Detection Model Design**:
    * We design a lightweight probe model using a Convolutional Neural Network (CNN) based on the **ResNet 18** architecture to capture intra-layer and inter-layer relationships.
    * The model is trained using a **Triplet Network structure** with triplet margin loss, which learns to minimize the distance between similar samples (e.g., two poisoned responses) and maximize the distance between different classes (poisoned vs. clean), enabling robust classification even with limited data.

## Key Results

* **High Detection Accuracy**: Across multiple benchmarks (Natural Questions, HotpotQA, MS-MARCO) and LLMs (Llama2, Mistral, GPT2-XL), RevPRAG consistently achieves >98% TPR.
* **Low False Alarm Rate**: The system maintains a low False Positive Rate (~1%), ensuring that correct RAG responses are rarely flagged as malicious.
* **Robustness**: The method proves effective against various retrieval strategies (Contriever, DPR, ANCE) and remains robust even when natural noise (typos, grammatical errors) is present in the text.
* **Efficiency**: Compared to methods like LLM Factoscope, RevPRAG incurs significantly lower computational costs for both training and inference, making it suitable for real-time applications.