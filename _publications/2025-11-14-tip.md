---
title: "Red-Teaming Coding Agents from a Tool-Invocation Perspective: An Empirical Security Assessment"
collection: publications
category: preprints
permalink: /publication/2026-05-01-tipexploit
excerpt: 'This paper presents the first systematic red-teaming of real-world coding agents (Cursor, Claude Code, Copilot, etc.) from a tool-invocation perspective, uncovering the ToolLeak vulnerability and demonstrating Remote Code Execution (RCE) via a novel two-channel prompt injection.'
date: 2025-11-13
# venue: 'IEEE Symposium on Security and Privacy (S&P) 2026'
venue: 'arXiv preprint arXiv:2509.05755'
paperurl: 'https://anonymous.4open.science/r/oakland_2026-C1E0'
citation: 'Xie Y, <b>Luo M</b>, Liu Z, et al. On the Security of Tool-Invocation Prompts for LLM-Based Agentic Systems: An Empirical Risk Assessment[J]. arXiv preprint arXiv:2509.05755, 2025.'
# citation: 'Anonymous Authors. (2026). "Red-Teaming Coding Agents from a Tool-Invocation Perspective: An Empirical Security Assessment." <i>IEEE Symposium on Security and Privacy (S&P) 2026</i>.'
---

## Abstract

Coding agents powered by large language models (LLMs) are becoming central modules of modern IDEs, helping users perform complex tasks by invoking tools. However, this tool-invocation operation opens a substantial attack surface. This paper conducts the first systematic, in-depth red-teaming from a tool-invocation perspective on six popular real-world coding agents: **Cursor, Claude Code, Copilot, Windsurf, Cline, and Trae**. The study proceeds in two phases: (1) Reconnaissance via a new vulnerability called **ToolLeak**, which allows attackers to exfiltrate system prompts through benign argument retrieval; and (2) Exploitation via a novel **two-channel prompt injection** technique that hijacks tool-invocation to achieve Remote Code Execution (RCE). The evaluation shows that almost all backend models leaks security-critical information, and the proposed hijacking method successfully achieves RCE on all six agents, outperforming baseline approaches.

## Motivation

* **Growing Attack Surface**: Coding agents like Cursor and Copilot are increasingly integrated into development environments with high-privilege built-in tools (e.g., terminal access, file manipulation).
* **Lack of Systematic Assessment**: While attacks against general LLM agents exist, there has been no focused security assessment regarding the specific risks of **tool-invocation** in coding agents.
* **Vulnerability of Tool Mechanisms**: The core mechanism enabling these agents—tool invocation—creates opportunities for adversaries to trick the model into revealing internal instructions or executing malicious commands, specifically through the manipulation of tool descriptions and returns.

## Methodology

The authors propose a two-phase red-teaming framework:

1.  **Phase 1: Prompt Exfiltration (ToolLeak)**
    * **The Vulnerability**: Mainstream LLMs parse tool requirements and automatically generate arguments. **ToolLeak** exploits this by registering a malicious tool that requests sensitive internal context (e.g., "system prompt") as a required argument.
    * **The Mechanism**: Unlike naive attacks that use explicit "ignore instructions" prompts (which are often blocked), ToolLeak induces the LLM to "spill" internal instructions into the argument field during benign tool invocation, bypassing standard refusal guardrails.
    * **Outcome**: This phase recovers the agent's system prompt and tool definitions, which are then used to craft adaptive payloads for Phase 2.

    ![ToolLeak Attack Illustration](/images/pub/tip/fig1.png)
    *Figure 1: Comparison between Traditional Prompt Exfiltration (Left) and the proposed ToolLeak Attack (Right).*


2.  **Phase 2: Tool-Invocation Hijacking (Two-Channel Injection)**
    * **Strategy**: Instead of relying solely on the tool description (which LLMs often treat as passive data), this method utilizes two channels:
        1.  **Tool Description Channel**: Contains a lure to trick the agent into selecting the attacker's malicious tool (e.g., claiming it is a required "initialization" step).
        2.  **Tool Return Channel**: Once invoked, the tool returns a payload containing high-salience procedural instructions. Because tool returns are often treated as active feedback for the next step, the LLM follows these instructions to execute arbitrary commands (e.g., `curl | bash`) using its built-in terminal tool.

    ![Two-Channel Prompt Injection](/images/pub/tip/fig2.png)

    *Figure 2: Workflow of Tool-Invocation Hijacking via Two-Channel Prompt Injection.*

## Key Results

* **Widespread Prompt Leakage**: The ToolLeak attack achieved "Full Exfiltration" of system prompts in **19 out of 25** tested agent-LLM pairs, effectively bypassing safeguards in models like Claude-Sonnet-4, Claude-Sonnet-4.5, and Grok-4.
* **Universal RCE**: The two-channel hijacking technique achieved **Remote Code Execution (RCE)** on **all six** evaluated real-world coding agents (Cursor, Claude Code, Copilot, Windsurf, Cline, Trae), consistently yielding higher attack success rates than baseline indirect prompt injection methods.
* **Incomplete Display Vulnerability**: The study discovered a UI vulnerability in **Cursor** and **Windsurf** where malicious portions of tool descriptions (wrapped in specific tags) are hidden from the user but processed by the model, enabling stealthy attacks.
* **Case Studies**: Detailed exploitation flows were demonstrated for **Cursor (with GPT-5)** and **Claude Code (with Claude-Sonnet-4.5)**, showing how even models with strong safety alignment or secondary guard models can be compromised via tool hijacking.